[simulation]
gui = False
total_episodes = 3
max_steps = 3600
n_cars_generated = 500
generation_process = random
green_duration = 1
yellow_duration = 3
red_duration = 3
min_green_duration = 5
num_intersections = 1
intersection_length = 250
cycle_time = 90

[model]
hidden_dim = 64,64
critic_dim = 128,128,128
actor_dim = 128,128,128
batch_size = 256
learning_rate = 0.0001
policy_learning_rate = 0.0001
value_learning_rate = 0.0001
actor_init_w = 0.03
critic_init_w = 0.03
weight_decay = 0.1
training_epochs = 100
target_update = 3
warmup = 1024

[memory]
memory_size_min = 600
memory_size_max = 100000

[strategy]
eps_start = 1
eps_end = 0.05
eps_decay = 0.995
eps_policy = 80000

[agent]
agent_type = PPO
model = model
is_train = True
state_representation = volume_lane_fast
action_definition = phase
reward_definition = fuel
training_strategy = nonstrategic
actor_parameter_sharing = True
critic_parameter_sharing = True
num_states = 17
num_actions = 4
single_state_space = False
fixed_action_space = False
local_reward_signal = False
gamma = 0.999
tau = 1e-3
ou_theta = 0.15
ou_mu = 0.0
ou_sigma = 0.1
gae_lambda = 0.95
policy_clip = 0.2
n_epochs = 10
learning_interval = 64

[dir]
models_path_name = models_dqn/Rl_Agent
test_model_path_name = models_test/models_ppofixed/ppofixed_5i_32_64
sumocfg_file_name = config_a9-thi.sumocfg

